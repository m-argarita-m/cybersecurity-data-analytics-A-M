{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7018a26",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "### Anirudh Margam\n",
    "### 730002982"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "882703cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fedf27",
   "metadata": {},
   "source": [
    "1. Create a TfidfVectorizer using the spam files in BG/2004 and ham files for kitchen-l from Canvas\n",
    "\n",
    "We first store all of the files into a list, so that we can pass them to the tfidf vectorizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dfc3fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_2004_directory_path = '../Datasets/BG/2004/'\n",
    "bg_2004_text_files = glob.glob(f'{bg_2004_directory_path}/*/*.txt')\n",
    "kitchen_directory_path = '../Datasets/kitchen-l/'\n",
    "kitchen_text_files = glob.glob(f'{kitchen_directory_path}/*/*')\n",
    "train_text_files = []\n",
    "train_text_files.extend(bg_2004_text_files)\n",
    "train_text_files.extend(kitchen_text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439bb7bd",
   "metadata": {},
   "source": [
    "Instantiate the TfidfVectorizer and call fit_transform using the spam and ham files. \n",
    "\n",
    "The fit_transform method is used for feature extraction and transformation, especially with text and numerical data. \n",
    "\n",
    "fit_transform performs 2 key actions:\n",
    "\n",
    "Fit: this part involves learning and capturing statistical information about the data. In regards to text data, it involves learning the vocabulary of words in the dataset. This information is stored in the model, enabling it to understand the data's characteristics.\n",
    "\n",
    "Transform: this part takes the learned information and applies it to the data. In regards to text data, it involves converting text into numerical features (TF-IDF scores). Effectively, it transforms the data based on insights gained during the 'fit' step.\n",
    "\n",
    "fit_transform ensures that the same transformations are consistently applied to both training and testing data, enabling accurate model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "baf43aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english', encoding='utf-8', decode_error='ignore')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(train_text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c757d5",
   "metadata": {},
   "source": [
    "2. Use a pandas DataFrame to look at the top 25 words in one spam message and the top 25 words in one ham message\n",
    "\n",
    "We instantiate the dataframe first. There were <4000 spam messages, so we can hardcode the indices for the spam message and ham message, and then use sort_values to sort the words in the message in descending order of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "556d4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "spam_message_index = 10\n",
    "top_25_spam_words = tfidf_df.iloc[spam_message_index].sort_values(ascending=False)[:25]\n",
    "ham_message_index = 4000\n",
    "top_25_ham_words = tfidf_df.iloc[ham_message_index].sort_values(ascending=False)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ad883601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words in a spam message:\n",
      "   word\t\ttfidf\n",
      "1. 82686       0.334887\n",
      "2. 4864605754  0.334887\n",
      "3. 0940002579  0.334887\n",
      "4. umbridol    0.308731\n",
      "5. cslwbmrbj   0.251165\n",
      "6. br          0.243594\n",
      "7. yahoo       0.183220\n",
      "8. un2u6       0.167443\n",
      "9. web34414    0.167443\n",
      "10. karmakamper 0.162016\n",
      "11. 122         0.141024\n",
      "12. 7781        0.137078\n",
      "13. 217         0.134364\n",
      "14. com         0.133636\n",
      "15. nov         0.125072\n",
      "16. www         0.119332\n",
      "17. unsub       0.114743\n",
      "18. ai          0.112224\n",
      "19. http        0.111166\n",
      "20. 200         0.100245\n",
      "21. 851129      0.083722\n",
      "22. 949270176157140.083722\n",
      "23. 12791       0.083722\n",
      "24. 21901       0.081008\n",
      "25. 2004        0.080072\n"
     ]
    }
   ],
   "source": [
    "print('Top 25 words in a spam message:')\n",
    "print('   word\\t\\ttfidf')\n",
    "for index, (word, tfidf) in enumerate(top_25_spam_words.items()):\n",
    "    print(f'{index+1}. {word.ljust(12)}{tfidf:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d3395ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words in a ham message:\n",
      "   word\t\ttfidf\n",
      "1. utilities   0.257595\n",
      "2. power       0.257414\n",
      "3. conservation0.188535\n",
      "4. sfgate      0.184800\n",
      "5. ect         0.167221\n",
      "6. speech      0.145621\n",
      "7. energy      0.138574\n",
      "8. plants      0.137258\n",
      "9. mike        0.135618\n",
      "10. grigsby     0.132139\n",
      "11. cut         0.130087\n",
      "12. percent     0.128174\n",
      "13. californians0.126858\n",
      "14. gov         0.124925\n",
      "15. holst       0.123078\n",
      "16. rates       0.123006\n",
      "17. generators  0.119938\n",
      "18. gray        0.119516\n",
      "19. gate        0.106892\n",
      "20. average     0.104604\n",
      "21. sf          0.100763\n",
      "22. prices      0.100652\n",
      "23. build       0.099846\n",
      "24. feds        0.096979\n",
      "25. california  0.092574\n"
     ]
    }
   ],
   "source": [
    "print('Top 25 words in a ham message:')\n",
    "print('   word\\t\\ttfidf')\n",
    "for index, (word, tfidf) in enumerate(top_25_ham_words.items()):\n",
    "    print(f'{index+1}. {word.ljust(12)}{tfidf:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c583f8",
   "metadata": {},
   "source": [
    "3. Train a RandomForestClassifier using the BG/2004 spam emails and kitchen-l ham Tfidf.\n",
    "\n",
    "We first create a list of the actual labels for the data, then split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c8c2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_labels = [1] * len(bg_2004_text_files) + [0] * len(kitchen_text_files)\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vector, actual_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83170b79",
   "metadata": {},
   "source": [
    "We then instantiate the Random Forest Classifier and train it on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cd042e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(max_depth=10, n_estimators=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=10, n_estimators=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(max_depth=10, n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f3903e",
   "metadata": {},
   "source": [
    "Now we can make predictions on the test data and calculate our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "77a64733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4df636",
   "metadata": {},
   "source": [
    "4. Using your RandomForestClassifier, predict the emails in BG/2005 and farmer-d. Display the number of true positive (spam), false positive, true negative (ham) and false negatives.\n",
    "\n",
    "We start by loading the emails from BG/2005 and farmer-d into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d655a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_2005_directory_path = '../Datasets/BG/2005/'\n",
    "bg_2005_text_files = glob.glob(f'{bg_2005_directory_path}/*/*.txt')\n",
    "farmer_directory_path = '../Datasets/farmer-d/'\n",
    "farmer_text_files = glob.glob(f'{farmer_directory_path}/*/*')\n",
    "test_text_files = []\n",
    "test_text_files.extend(bg_2005_text_files)\n",
    "test_text_files.extend(farmer_text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be9bae",
   "metadata": {},
   "source": [
    "Next, we call transform on the existing tfidf vectorizer to transform the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "89ac1b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_tfidf = tfidf_vectorizer.transform(test_text_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89193e6",
   "metadata": {},
   "source": [
    "Now we can make predictions on the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ee63a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_predictions = rf_classifier.predict(new_data_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd6de6",
   "metadata": {},
   "source": [
    "We create our list of actual labels for the new data and output the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7c92765e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3665    4]\n",
      " [   2 6120]]\n",
      "\n",
      "Number of True Positives (Spam):\t3665\n",
      "Number of False Positives:\t\t4\n",
      "Number of True Negatives (Ham):\t\t6120\n",
      "Number of False Negatives:\t\t2\n"
     ]
    }
   ],
   "source": [
    "actual_labels = [1] * len(bg_2005_text_files) + [0] * len(farmer_text_files)\n",
    "cm = confusion_matrix(actual_labels, new_data_predictions)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "tp = cm[0][0]\n",
    "tn = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "print(f'\\nNumber of True Positives (Spam):\\t{tp}')\n",
    "print(f'Number of False Positives:\\t\\t{fp}')\n",
    "print(f'Number of True Negatives (Ham):\\t\\t{tn}')\n",
    "print(f'Number of False Negatives:\\t\\t{fn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8c16d0",
   "metadata": {},
   "source": [
    "Evidently, our Random Forest Classifier is extremely accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e2597",
   "metadata": {},
   "source": [
    "5. In the second half of your Jupyter notebook (i.e. don’t change your code above, add more), redo the steps above using a stopwords list containing “enron” and HTML tags. Again, predict the emails in BG/2005 and farmer-d. Display the number of true positive (spam), false positive, true negative (ham) and false negatives. (It will probably still be outstanding).\n",
    "\n",
    "We start by creating a list of stopwords using the github link and the list of HTML tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2b260982",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_of_stopwords = '../Datasets/stopwords_list.txt'\n",
    "stopwords_list = ['enron']\n",
    "\n",
    "with open(text_file_of_stopwords, 'r') as file:\n",
    "    for line in file:\n",
    "        word = line.strip()\n",
    "        stopwords_list.append(word)\n",
    "\n",
    "text_file_of_html_tags = '../Datasets/list_of_html_tags.txt'\n",
    "with open(text_file_of_html_tags, 'r') as file:\n",
    "    input_text = file.read()\n",
    "\n",
    "html_tag_regex = r'<[^>]+>'\n",
    "html_tags_list = re.findall(html_tag_regex, input_text)\n",
    "cleaned_html_tags = [tag.strip('<>') for tag in html_tags_list]\n",
    "\n",
    "stopwords_list.extend(cleaned_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ae9b5",
   "metadata": {},
   "source": [
    "Now that we have our list of stopwords, we can instantiate a new TfidfVectorizer and fit it against our training files. Then we can transform it according to our test files and make predictions again using our Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3b14af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'daren', 'doctype', 'hadn', 'herse', 'himse', 'itse', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[3659   10]\n",
      " [   0 6122]]\n",
      "\n",
      "Number of True Positives (Spam):\t3659\n",
      "Number of False Positives:\t\t10\n",
      "Number of True Negatives (Ham):\t\t6122\n",
      "Number of False Negatives:\t\t0\n"
     ]
    }
   ],
   "source": [
    "new_tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words=stopwords_list, encoding='utf-8', decode_error='ignore')\n",
    "new_tfidf_vectorizer.fit_transform(train_text_files)\n",
    "new_data_tfidf = new_tfidf_vectorizer.transform(test_text_files)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_data_tfidf, actual_labels, test_size=0.2, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "new_data_predictions = rf_classifier.predict(new_data_tfidf)\n",
    "\n",
    "cm = confusion_matrix(actual_labels, new_data_predictions)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n",
    "tp = cm[0][0]\n",
    "tn = cm[1][1]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "print(f'\\nNumber of True Positives (Spam):\\t{tp}')\n",
    "print(f'Number of False Positives:\\t\\t{fp}')\n",
    "print(f'Number of True Negatives (Ham):\\t\\t{tn}')\n",
    "print(f'Number of False Negatives:\\t\\t{fn}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef82e6e",
   "metadata": {},
   "source": [
    "The performance is still very good. We get a few more false positives, but no false negatives, which is better performance than before - in a real world context, false negatives are more costly than false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "82cb7916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 words in a spam message:\n",
      "   word\t\ttfidf\n",
      "1. 2005        0.357249\n",
      "2. mar         0.298718\n",
      "3. bulgaria    0.292137\n",
      "4. elle        0.248318\n",
      "5. music       0.246069\n",
      "6. 135         0.220193\n",
      "7. bone        0.207728\n",
      "8. 156         0.201008\n",
      "9. refi        0.165545\n",
      "10. 218         0.162617\n",
      "11. dyndns      0.150727\n",
      "12. guenter     0.150579\n",
      "13. 05          0.141412\n",
      "14. bruce       0.141037\n",
      "15. 41          0.138585\n",
      "16. 12          0.133674\n",
      "17. 127         0.107921\n",
      "18. localhost   0.106446\n",
      "19. received    0.099903\n",
      "20. 26915       0.095690\n",
      "21. host76      0.095690\n",
      "22. rait        0.094729\n",
      "23. 0000        0.088127\n",
      "24. approval    0.086336\n",
      "25. homeowner   0.084362\n"
     ]
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(new_data_tfidf.toarray(), columns=new_tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "spam_message_index = 10\n",
    "top_25_spam_words = tfidf_df.iloc[spam_message_index].sort_values(ascending=False)[:25]\n",
    "\n",
    "print('Top 25 words in a spam message:')\n",
    "print('   word\\t\\ttfidf')\n",
    "for index, (word, tfidf) in enumerate(top_25_spam_words.items()):\n",
    "    print(f'{index+1}. {word.ljust(12)}{tfidf:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd7b8e9",
   "metadata": {},
   "source": [
    "The code above is used to verify the functionality of the stopwords. We see that the top 25 words in a spam message are more useful now, since we aren't seeing any HTML tags (ex: br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a691dcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
